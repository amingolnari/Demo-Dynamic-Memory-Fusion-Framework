{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [**Adaptive Real-Time Multi-Loss Function Optimization Using Dynamic Memory Fusion Framework: A Case Study on Breast Cancer Segmentation**](https://arxiv.org/pdf/2410.19745)  \n",
        "\n",
        "---\n",
        "\n",
        "Amin Golnari, Mostafa Diba. **\"Adaptive Real-Time Multi-Loss Function Optimization Using Dynamic Memory Fusion Framework: A Case Study on Breast Cancer Segmentation\"** Preprint. [https://doi.org/10.48550/arXiv.2410.19745](https://doi.org/10.48550/arXiv.2410.19745)\n",
        "\n",
        "\n",
        "**Amin Golnari <sup>a<sup>**, **Mostafa Diba <sup>a<sup>** <br>\n",
        "a) Faculty of Electrical Engineering, Shahrood University of Technology, Shahrood, Iran <br>\n",
        "\n",
        "**Link to the preprint version:** [Click here](https://arxiv.org/pdf/2410.19745)\n",
        "\n",
        "**Or You Can Run this Python Code on Google Colab:**    \n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amingolnari/Demo-Dynamic-Memory-Fusion-Framework/blob/main/DynamicMemoryFusion.ipynb)\n",
        "\n",
        "**Compatibility:**\n",
        "- TensorFlow: 2.17.0\n",
        "- Keras: 3.4.1\n",
        "- TensorFlow Datasets: 4.9.6\n",
        "\n",
        "---\n",
        "\n",
        "## **Framework Overview**\n",
        "\n",
        "The **Dynamic Memory Fusion Framework** is an adaptive, real-time, multi-loss functions optimization framework designed to improve deep learning task performance.\n",
        "\n",
        "This framework is based on deep learning and addresses the inefficiencies of manual tuning in multi-loss functions by dynamically adjusting loss weights during training. It utilizes historical loss data to update weights and integrates auxiliary loss functions that enhance model performance, especially in the early stages of training. Additionally, the **Class-Balanced Dice Loss** function is introduced to address the issue of class imbalance, which is crucial for accurate segmentation tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Features:**\n",
        "\n",
        "1. **Dynamic Weight Adjustment**:\n",
        "   - The framework dynamically adjusts the weighting of multiple loss functions in real time based on historical loss values.\n",
        "   - This adaptation ensures better performance across different training stages without the need for manual fine-tuning.\n",
        "\n",
        "2. **Class-Balanced Dice Loss (CB-Dice)**:\n",
        "   - A novel loss function that handles class imbalance by focusing more on underrepresented classes, improving the overall segmentation accuracy.\n",
        "\n",
        "3. **Auxiliary Loss Functions**:\n",
        "   - Auxiliary loss functions are employed to assist in the early stages of training, helping the model converge faster.\n",
        "\n",
        "4. **Breast Ultrasound Dataset**:\n",
        "   - The framework has been tested on breast ultrasound datasets, demonstrating improvements in segmentation accuracy and robustness across various metrics, such as Dice score and IoU (Intersection over Union).\n",
        "\n",
        "---\n",
        "\n",
        "## **Usage of CB-Dice Loss**\n",
        "\n",
        "The **CB-Dice Loss** function is crucial in image segmentation where there is often a significant class imbalance. In this framework, the Dice loss function has been modified to prioritize underrepresented classes, ensuring better accuracy for minority class segmentation (e.g., cancerous regions in medical images).\n",
        "\n",
        "---\n",
        "\n",
        "If our work is helpful to you, please kindly cite our paper as:\n",
        "\n",
        "    @article{GOLNARI2024DMF,\n",
        "       title={Adaptive Real-Time Multi-Loss Function Optimization Using Dynamic Memory Fusion Framework: A Case Study on Breast Cancer Segmentation},\n",
        "       author={Golnari, Amin and Diba, Mostafa},\n",
        "       journal={arXiv preprint arXiv:2410.19745},\n",
        "       year={2024},\n",
        "       doi={https://doi.org/10.48550/arXiv.2410.19745}\n",
        "    }\n"
      ],
      "metadata": {
        "id": "NwOHsNuPW7gS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFGQIAEavIKX"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os         # For handling file and directory operations\n",
        "import shutil     # For performing high-level file operations like copying and deleting\n",
        "\n",
        "# Deep learning frameworks\n",
        "import keras             # High-level neural networks API running on top of TensorFlow\n",
        "import tensorflow as tf  # TensorFlow is an open-source machine learning library\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt  # For plotting graphs and visualizing data\n",
        "\n",
        "# TensorFlow datasets\n",
        "import tensorflow_datasets as tfds  # Library of ready-to-use datasets for machine learning\n",
        "\n",
        "# Numerical operations\n",
        "import numpy as np  # For efficient array and numerical operations\n",
        "\n",
        "# Specific modules from Keras\n",
        "from keras import layers, models  # For defining layers and building models in Keras\n",
        "from keras import backend as K    # Low-level tensor manipulation and backend operations in Keras\n",
        "\n",
        "# Scikit-learn's train_test_split utility\n",
        "from sklearn.model_selection import train_test_split  # For splitting datasets into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics:\n",
        "    \"\"\"\n",
        "    A class to compute various evaluation metrics and loss functions for classification models,\n",
        "    supporting both binary and multi-class scenarios. It leverages TensorFlow and Keras backend\n",
        "    for efficient computation and integration with neural network models.\n",
        "    \"\"\"\n",
        "\n",
        "    def _confusion_matrix_elements(self, y_true, y_pred, class_id, is_binary, across_batch=False):\n",
        "        \"\"\"\n",
        "        Computes the elements of the confusion matrix (True Positives, False Positives,\n",
        "        True Negatives, False Negatives) for a specific class.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "            class_id (int): The class identifier for which to compute the confusion matrix elements.\n",
        "            is_binary (bool): Flag indicating if the classification is binary.\n",
        "            across_batch (bool): Whether to compute metrics across the entire batch.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of Tensors: (TP, FP, TN, FN)\n",
        "        \"\"\"\n",
        "        if is_binary:\n",
        "            # Threshold predictions for binary classification\n",
        "            y_pred = tf.where(y_pred >= 0.5, 1.0, 0.0)\n",
        "            y_pred_binary = tf.cast(y_pred == class_id, tf.float32)\n",
        "            y_true_binary = tf.cast(y_true == class_id, tf.float32)\n",
        "        else:\n",
        "            # For multi-class classification, use argmax to determine predicted class\n",
        "            y_true_binary = tf.cast(tf.argmax(y_true, axis=-1) == class_id, tf.float32)\n",
        "            y_pred_binary = tf.cast(tf.argmax(y_pred, axis=-1) == class_id, tf.float32)\n",
        "\n",
        "        # Determine the shape of the tensors to handle different dimensions\n",
        "        if tf.rank(y_true_binary) == 2:  # [height, width]\n",
        "            TP = tf.reduce_sum(tf.cast((y_true_binary == 1) & (y_pred_binary == 1), tf.float32))\n",
        "            FP = tf.reduce_sum(tf.cast((y_true_binary == 0) & (y_pred_binary == 1), tf.float32))\n",
        "            TN = tf.reduce_sum(tf.cast((y_true_binary == 0) & (y_pred_binary == 0), tf.float32))\n",
        "            FN = tf.reduce_sum(tf.cast((y_true_binary == 1) & (y_pred_binary == 0), tf.float32))\n",
        "        else:\n",
        "            # For 3D tensors (batch, height, width), reduce across spatial dimensions\n",
        "            TP = tf.reduce_sum(tf.cast((y_true_binary == 1) & (y_pred_binary == 1), tf.float32), axis=[1, 2])\n",
        "            FP = tf.reduce_sum(tf.cast((y_true_binary == 0) & (y_pred_binary == 1), tf.float32), axis=[1, 2])\n",
        "            TN = tf.reduce_sum(tf.cast((y_true_binary == 0) & (y_pred_binary == 0), tf.float32), axis=[1, 2])\n",
        "            FN = tf.reduce_sum(tf.cast((y_true_binary == 1) & (y_pred_binary == 0), tf.float32), axis=[1, 2])\n",
        "\n",
        "            if not across_batch:\n",
        "                # Average over the batch dimension\n",
        "                TP = tf.reduce_mean(TP)\n",
        "                FP = tf.reduce_mean(FP)\n",
        "                TN = tf.reduce_mean(TN)\n",
        "                FN = tf.reduce_mean(FN)\n",
        "\n",
        "        return TP, FP, TN, FN\n",
        "\n",
        "    def _get_num_classes(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Determines the number of classes and whether the classification is binary.\n",
        "\n",
        "        Args:\n",
        "            y_true (numpy.ndarray): Ground truth labels.\n",
        "            y_pred (numpy.ndarray): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (number_of_classes, is_binary)\n",
        "        \"\"\"\n",
        "        if np.shape(y_pred)[-1] == 1:  # Binary classification with sigmoid output\n",
        "            return 2, True\n",
        "        elif np.shape(y_pred)[-1] == 2:  # Binary classification with softmax output\n",
        "            return 2, False\n",
        "        else:  # Multi-class classification with softmax output\n",
        "            return np.shape(y_pred)[-1], False\n",
        "\n",
        "    def _compute_metric(self, y_true, y_pred, metric_fn, across_classes=True):\n",
        "        \"\"\"\n",
        "        Computes a specified metric across all classes.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "            metric_fn (function): Function to compute the metric based on TP, FP, TN, FN.\n",
        "            across_classes (bool): Whether to average the metric across classes.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Computed metric.\n",
        "        \"\"\"\n",
        "        num_classes, is_binary = self._get_num_classes(y_true, y_pred)\n",
        "        metric_sum = 0.0\n",
        "\n",
        "        if across_classes:\n",
        "            # Aggregate metric across all classes\n",
        "            for class_id in range(num_classes):\n",
        "                TP, FP, TN, FN = self._confusion_matrix_elements(y_true, y_pred, class_id, is_binary)\n",
        "                metric_sum += metric_fn(TP, FP, TN, FN)\n",
        "\n",
        "            return metric_sum / tf.cast(num_classes, tf.float32)\n",
        "        else:\n",
        "            # Compute metric for each class individually\n",
        "            metric = []\n",
        "            for class_id in range(num_classes):\n",
        "                TP, FP, TN, FN = self._confusion_matrix_elements(y_true, y_pred, class_id, is_binary)\n",
        "                metric.append(metric_fn(TP, FP, TN, FN))\n",
        "\n",
        "            return metric\n",
        "\n",
        "    # ----------------------- Evaluation Metrics -----------------------\n",
        "\n",
        "    def precision(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the precision metric.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Precision score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: TP / (TP + FP + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def recall(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the recall metric.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Recall score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: TP / (TP + FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def f1_score(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the F1 score, the harmonic mean of precision and recall.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: F1 score.\n",
        "        \"\"\"\n",
        "        precision_value = self.precision(y_true, y_pred)\n",
        "        recall_value = self.recall(y_true, y_pred)\n",
        "        return 2 * (precision_value * recall_value) / (precision_value + recall_value + K.epsilon())\n",
        "\n",
        "    def accuracy(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the accuracy metric.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Accuracy score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: (TP + TN) / (TP + FP + TN + FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def dice(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the Dice coefficient.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Dice coefficient.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: (2 * TP) / (2 * TP + FP + FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def iou(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the Intersection over Union (IoU) metric.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: IoU score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: TP / (TP + FP + FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def specificity(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the specificity metric.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Specificity score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: TN / (TN + FP + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def sensitivity(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the sensitivity metric, equivalent to recall.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Sensitivity score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: TP / (TP + FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def fp_rate(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the False Positive Rate (FPR).\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: FPR score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: FP / (FP + TN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def fn_rate(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the False Negative Rate (FNR).\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: FNR score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: FN / (FN + TP + K.epsilon())\n",
        "        )\n",
        "\n",
        "    def negative_predictive(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the Negative Predictive Value (NPV).\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: NPV score.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: TN / (TN + FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    # ----------------------- Loss Functions -----------------------\n",
        "\n",
        "    def dice_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the Dice loss, which is 1 minus the Dice coefficient.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Dice loss.\n",
        "        \"\"\"\n",
        "        return 1 - self.dice(y_true, y_pred)\n",
        "\n",
        "    def iou_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the IoU loss, which is 1 minus the IoU score.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: IoU loss.\n",
        "        \"\"\"\n",
        "        return 1 - self.iou(y_true, y_pred)\n",
        "\n",
        "    def precision_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the Precision loss, which is 1 minus the precision score.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Precision loss.\n",
        "        \"\"\"\n",
        "        return 1 - self.precision(y_true, y_pred)\n",
        "\n",
        "    def recall_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the Recall loss, which is 1 minus the recall score.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Recall loss.\n",
        "        \"\"\"\n",
        "        return 1 - self.recall(y_true, y_pred)\n",
        "\n",
        "    def f1_score_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the F1 score loss, which is 1 minus the F1 score.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: F1 score loss.\n",
        "        \"\"\"\n",
        "        return 1 - self.f1_score(y_true, y_pred)\n",
        "\n",
        "    def focal_loss(self, y_true, y_pred, gamma=2.0, alpha=0.25):\n",
        "        \"\"\"\n",
        "        Computes the Focal Loss to address class imbalance.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels (probabilities).\n",
        "            gamma (float): Focusing parameter that adjusts the rate at which easy examples are down-weighted.\n",
        "            alpha (float): Weighting factor for the rare class.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Focal Loss value.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: -alpha * tf.pow(1 - TP / (TP + FP + K.epsilon()), gamma) * tf.math.log(TP / (TP + FP + K.epsilon()))\n",
        "        )\n",
        "\n",
        "\n",
        "    def tversky_loss(self, y_true, y_pred, alpha=0.7, beta=0.3):\n",
        "        \"\"\"\n",
        "        Computes the Tversky loss for imbalanced data.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "            alpha (float): Weight for false positives.\n",
        "            beta (float): Weight for false negatives.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Tversky loss value.\n",
        "        \"\"\"\n",
        "        return self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: 1 - (TP + K.epsilon()) / (TP + alpha * FP + beta * FN + K.epsilon())\n",
        "        )\n",
        "\n",
        "    # ----------------------- Class Balanced Dice Metric and Loss -----------------------\n",
        "\n",
        "    def _compute_class_weights(self, y_true):\n",
        "        \"\"\"\n",
        "        Computes class weights based on the frequency of each class in the ground truth.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Normalized class weights.\n",
        "        \"\"\"\n",
        "        epsilon = 1e-6\n",
        "        total_pixels = tf.reduce_prod(tf.shape(y_true)[:-1])\n",
        "        class_pixel_counts = tf.reduce_sum(y_true, axis=[0, 1, 2])\n",
        "        class_ratios = class_pixel_counts / tf.cast(total_pixels, tf.float32)\n",
        "        class_weights = 1 / (class_ratios + epsilon)\n",
        "        class_weights = class_weights / np.sum(class_weights)  # Normalize weights\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    def class_balanced_dice_score(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Computes a class-balanced Dice score by weighting each class's Dice score.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Class-balanced Dice score.\n",
        "        \"\"\"\n",
        "        # Compute Dice score for each class without averaging across classes\n",
        "        loss_score = self._compute_metric(\n",
        "            y_true, y_pred,\n",
        "            lambda TP, FP, TN, FN: (2 * TP) / (2 * TP + FP + FN + K.epsilon()),\n",
        "            across_classes=False\n",
        "        )\n",
        "        class_weights = self._compute_class_weights(y_true)\n",
        "        score = tf.reduce_sum(class_weights * loss_score)  # Weighted sum of Dice score\n",
        "        return score\n",
        "\n",
        "    def class_balanced_dice_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates the class-balanced Dice loss.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Class-balanced Dice loss.\n",
        "        \"\"\"\n",
        "        return 1 - self.class_balanced_dice_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "# Instantiate the Metrics class\n",
        "metrics_instance = Metrics()\n",
        "\n",
        "# ----------------------- Metric Functions -----------------------\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute precision using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.precision(y_true, y_pred)\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute recall using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.recall(y_true, y_pred)\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute F1 score using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.f1_score(y_true, y_pred)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute accuracy using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.accuracy(y_true, y_pred)\n",
        "\n",
        "def dice(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Dice coefficient using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.dice(y_true, y_pred)\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Intersection over Union (IoU) using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.iou(y_true, y_pred)\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute specificity using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.specificity(y_true, y_pred)\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute sensitivity using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.sensitivity(y_true, y_pred)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Dice loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.dice_loss(y_true, y_pred)\n",
        "\n",
        "def iou_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute IoU loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.iou_loss(y_true, y_pred)\n",
        "\n",
        "def precision_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Precision loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.precision_loss(y_true, y_pred)\n",
        "\n",
        "def recall_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Recall loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.recall_loss(y_true, y_pred)\n",
        "\n",
        "def f1_score_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute F1 score loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.f1_score_loss(y_true, y_pred)\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Tversky loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.tversky_loss(y_true, y_pred)\n",
        "\n",
        "def focal_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute Focal loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.focal_loss(y_true, y_pred)\n",
        "\n",
        "def class_balanced_dice_score(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute class-balanced Dice score using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.class_balanced_dice_score(y_true, y_pred)\n",
        "\n",
        "def class_balanced_dice_loss(y_true, y_pred):\n",
        "    \"\"\"Wrapper function to compute class-balanced Dice loss using the Metrics instance.\"\"\"\n",
        "    return metrics_instance.class_balanced_dice_loss(y_true, y_pred)"
      ],
      "metadata": {
        "id": "l9OKb__R83sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicMemoryFusionLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    A custom Keras loss function that dynamically fuses multiple loss functions\n",
        "    by adjusting their weights based on historical performance. This approach\n",
        "    aims to balance the contribution of each loss function during training,\n",
        "    enhancing model performance especially in complex tasks with multiple objectives.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 loss_functions,\n",
        "                 initial_loss_weights=None,\n",
        "                 history_size=50,\n",
        "                 history_size_max=100,\n",
        "                 batch_track_steps=None,\n",
        "                 gama=1,\n",
        "                 decay_rate=0.05,\n",
        "                 auxiliary_loss_function=None,\n",
        "                 weighting_method='var',\n",
        "                 training_phase=True,\n",
        "                 name=\"DynamicMemoryFusionLoss\",\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the DynamicMemoryFusionLoss.\n",
        "\n",
        "        Args:\n",
        "            loss_functions (list of callable): A list of loss functions to be fused.\n",
        "            initial_loss_weights (list of float, optional): Initial weights for each loss function.\n",
        "                If None, weights are initialized uniformly.\n",
        "            history_size (int, optional): The number of past loss values to consider for adjusting weights.\n",
        "                Minimum is set to 30.\n",
        "            history_size_max (int, optional): The maximum number of historical loss values to store.\n",
        "                Minimum is set to 90.\n",
        "            batch_track_steps (int, optional): Number of steps between tracking updates.\n",
        "                If None, tracking occurs every step.\n",
        "            gama (float, optional): Scaling factor for the auxiliary loss.\n",
        "            decay_rate (float, optional): Decay rate for scaling the auxiliary loss over steps.\n",
        "            auxiliary_loss_function (callable, optional): An additional loss function to include.\n",
        "            weighting_method (str, optional): Method for adjusting loss weights. Options are\n",
        "                'var' (variance-based), 'bayes', or 'mad' (median absolute deviation). Defaults to 'var'.\n",
        "            training_phase (bool, optional): Flag indicating if the loss is being used in training phase.\n",
        "                If False, weight adjustments are not performed.\n",
        "            name (str, optional): Name of the loss function.\n",
        "            **kwargs: Additional keyword arguments for the parent class.\n",
        "        \"\"\"\n",
        "        super(DynamicMemoryFusionLoss, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        # Store the loss functions and initialize their weights\n",
        "        self.loss_functions = loss_functions\n",
        "        if initial_loss_weights is not None:\n",
        "            self.loss_weights = initial_loss_weights\n",
        "        else:\n",
        "            # Initialize weights uniformly if not provided\n",
        "            self.loss_weights = [1.0 / len(loss_functions)] * len(loss_functions)\n",
        "\n",
        "        # Set history size parameters with minimum constraints\n",
        "        self.history_size = max(30, history_size)\n",
        "        self.history_size_max = max(90, history_size_max)\n",
        "\n",
        "        # Initialize batch tracking parameters\n",
        "        self.batch_track_steps = 0 if batch_track_steps is None else batch_track_steps\n",
        "        self.batch_track = self.batch_track_steps\n",
        "\n",
        "        # Set scaling and decay parameters\n",
        "        self.gama = gama\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "        # Auxiliary loss function and weighting method\n",
        "        self.auxiliary_loss_function = auxiliary_loss_function\n",
        "        if weighting_method in ['var', 'bayes', 'mad']:\n",
        "            self.weighting_method = weighting_method\n",
        "        else:\n",
        "            self.weighting_method = 'mad'  # Default to 'mad' if invalid method is provided\n",
        "\n",
        "        # Flag to indicate if in training phase\n",
        "        self.training_phase = training_phase\n",
        "\n",
        "        # Initialize priors for Bayesian weighting\n",
        "        self.priors = np.array([1.0 / len(loss_functions)] * len(loss_functions))\n",
        "\n",
        "        # Initialize history storage for each loss function\n",
        "        self.loss_histories = [[] for _ in loss_functions]\n",
        "\n",
        "        # Initialize step counter\n",
        "        self.step = 0\n",
        "\n",
        "    def _ema(self, values, alpha=0.9):\n",
        "        \"\"\"\n",
        "        Computes the Exponential Moving Average (EMA) of the provided values.\n",
        "\n",
        "        Args:\n",
        "            values (Tensor): Tensor of values to compute EMA on.\n",
        "            alpha (float, optional): Smoothing factor for EMA. Defaults to 0.9.\n",
        "\n",
        "        Returns:\n",
        "            float: The EMA of the values.\n",
        "        \"\"\"\n",
        "        values = values.numpy()\n",
        "        ema_value = values[0]\n",
        "        for i in range(1, len(values)):\n",
        "            ema_value = alpha * values[i] + (1 - alpha) * ema_value\n",
        "\n",
        "        return ema_value\n",
        "\n",
        "    def _batch_track(self):\n",
        "        \"\"\"\n",
        "        Manages the tracking of batches to control when weight adjustments occur.\n",
        "        \"\"\"\n",
        "        if self.batch_track_steps:\n",
        "            self.batch_track -= 1\n",
        "            if self.batch_track == 0:\n",
        "                self.batch_track = self.batch_track_steps\n",
        "                self.step += 1\n",
        "        else:\n",
        "            self.step += 1\n",
        "\n",
        "    def _update_loss_histories(self, loss_values):\n",
        "        \"\"\"\n",
        "        Updates the historical loss values for each loss function.\n",
        "\n",
        "        Args:\n",
        "            loss_values (list of float): Current loss values for each loss function.\n",
        "        \"\"\"\n",
        "        for i, loss_value in enumerate(loss_values):\n",
        "            self.loss_histories[i].append(loss_value)\n",
        "            # Ensure the history does not exceed the maximum size\n",
        "            if len(self.loss_histories[i]) > self.history_size_max:\n",
        "                self.loss_histories[i].pop(0)\n",
        "\n",
        "    def _compute_median(self, tensor):\n",
        "        \"\"\"\n",
        "        Computes the median of a tensor.\n",
        "\n",
        "        Args:\n",
        "            tensor (Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Median value.\n",
        "        \"\"\"\n",
        "        tensor = tf.reshape(tensor, [-1])\n",
        "        sorted_tensor = tf.sort(tensor)\n",
        "        num_elements = tf.size(sorted_tensor)\n",
        "        is_even = tf.math.mod(num_elements, 2)\n",
        "        median_idx = num_elements // 2\n",
        "        if is_even:\n",
        "            # Average the two middle values for even-sized tensors\n",
        "            median = tf.reduce_mean([sorted_tensor[median_idx - 1], sorted_tensor[median_idx]])\n",
        "        else:\n",
        "            # Middle value for odd-sized tensors\n",
        "            median = sorted_tensor[median_idx]\n",
        "\n",
        "        return median\n",
        "\n",
        "    def _compute_mad(self, loss_history):\n",
        "        \"\"\"\n",
        "        Computes the Median Absolute Deviation (MAD) of a loss history.\n",
        "\n",
        "        Args:\n",
        "            loss_history (Tensor): Tensor containing historical loss values.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: MAD value.\n",
        "        \"\"\"\n",
        "        median = self._compute_median(loss_history)\n",
        "        abs_deviations = tf.math.abs(loss_history - median)\n",
        "        mad = self._compute_median(abs_deviations)\n",
        "\n",
        "        return mad\n",
        "\n",
        "    def _minmax_scaling(self, loss):\n",
        "        \"\"\"\n",
        "        Applies min-max scaling to the loss values.\n",
        "\n",
        "        Args:\n",
        "            loss (Tensor): Tensor of loss values.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Scaled loss values between 0 and 1.\n",
        "        \"\"\"\n",
        "        min_val = tf.reduce_min(loss)\n",
        "        max_val = tf.reduce_max(loss)\n",
        "        scaled_loss = (loss - min_val) / (max_val - min_val + K.epsilon())\n",
        "\n",
        "        return scaled_loss\n",
        "\n",
        "    def _symmetric_log_scaling(self, loss):\n",
        "        \"\"\"\n",
        "        Applies symmetric logarithmic scaling to the loss values.\n",
        "\n",
        "        Args:\n",
        "            loss (Tensor): Tensor of loss values.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Scaled loss values with logarithmic transformation.\n",
        "        \"\"\"\n",
        "        positive_scaled = tf.math.log1p(loss)      # log(1 + loss) for positive values\n",
        "        negative_scaled = -tf.math.log1p(-loss)   # -log(1 + (-loss)) for negative values\n",
        "        scaled_loss = tf.where(loss >= 0, positive_scaled, negative_scaled)\n",
        "\n",
        "        return scaled_loss\n",
        "\n",
        "    def _adjust_loss_weights(self):\n",
        "        \"\"\"\n",
        "        Adjusts the weights of each loss function based on their historical performance\n",
        "        using the specified weighting method.\n",
        "        \"\"\"\n",
        "        if self.weighting_method == 'var':\n",
        "            if len(self.loss_histories[0]) >= self.history_size:\n",
        "                normalized_histories = []\n",
        "                for history in self.loss_histories:\n",
        "                    history_tensor = tf.convert_to_tensor(history)\n",
        "                    normalized_log = self._symmetric_log_scaling(history_tensor)\n",
        "                    normalized_minmax = self._minmax_scaling(normalized_log)\n",
        "                    normalized_histories.append(normalized_minmax)\n",
        "\n",
        "                # Compute variance for each normalized loss history\n",
        "                variances = [tf.math.reduce_variance(history) for history in normalized_histories]\n",
        "\n",
        "                if len(variances) == len(self.loss_functions):\n",
        "                    total_variance = tf.reduce_sum(variances)\n",
        "                    normalized_weights = [v / total_variance for v in variances]\n",
        "                    self.loss_weights = normalized_weights\n",
        "\n",
        "        elif self.weighting_method in ['bayes', 'mad']:\n",
        "            if len(self.loss_histories[0]) >= self.history_size:\n",
        "                normalized_histories = []\n",
        "                for history in self.loss_histories:\n",
        "                    history_tensor = tf.convert_to_tensor(history)\n",
        "                    normalized_log = self._symmetric_log_scaling(history_tensor)\n",
        "                    normalized_minmax = self._minmax_scaling(normalized_log)\n",
        "                    normalized_histories.append(normalized_minmax)\n",
        "\n",
        "                # Compute MAD for each normalized loss history\n",
        "                mad_values = tf.stack([self._compute_mad(history) for history in normalized_histories])\n",
        "\n",
        "                if len(mad_values) == len(self.loss_functions):\n",
        "                    if self.weighting_method == 'bayes':\n",
        "                        # Bayesian weighting: likelihood * priors normalized\n",
        "                        likelihoods = 1.0 / (mad_values + K.epsilon())\n",
        "                        posteriors = likelihoods * self.priors\n",
        "                        posteriors /= tf.reduce_sum(posteriors)\n",
        "                        self.loss_weights = posteriors\n",
        "                        self.priors = posteriors  # Update priors\n",
        "                    else:\n",
        "                        # MAD-based weighting: inverse MAD normalized\n",
        "                        self.loss_weights = 1.0 / (mad_values + K.epsilon())\n",
        "                        self.loss_weights /= tf.reduce_sum(self.loss_weights)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Computes the dynamic fused loss.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Ground truth labels.\n",
        "            y_pred (Tensor): Predicted labels.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The computed dynamic fused loss.\n",
        "        \"\"\"\n",
        "        # Compute individual loss values from each loss function\n",
        "        loss_values = [loss_function(y_true, y_pred) for loss_function in self.loss_functions]\n",
        "\n",
        "        if self.training_phase:\n",
        "            # Update loss histories with current loss values\n",
        "            self._update_loss_histories(loss_values)\n",
        "            # Adjust loss weights based on historical performance\n",
        "            self._adjust_loss_weights()\n",
        "\n",
        "        # Compute the weighted sum of loss values\n",
        "        weighted_loss = tf.reduce_sum([w * loss for w, loss in zip(self.loss_weights, loss_values)])\n",
        "\n",
        "        # Incorporate auxiliary loss if provided\n",
        "        if self.auxiliary_loss_function is not None:\n",
        "            auxiliary_loss = self.auxiliary_loss_function(y_true, y_pred)\n",
        "            # Apply scaling and decay to the auxiliary loss\n",
        "            weighted_loss += (self.gama * tf.math.exp(-self.decay_rate * self.step)) * auxiliary_loss\n",
        "\n",
        "            if self.training_phase:\n",
        "                # Update batch tracking for auxiliary loss\n",
        "                self._batch_track()\n",
        "\n",
        "        return weighted_loss"
      ],
      "metadata": {
        "id": "Vc04Rz3f7tuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function defining a downsampling block with MaxPooling2D\n",
        "def downsampling_block(x, filters, kernel_size=3, padding='same'):\n",
        "    \"\"\"\n",
        "    Applies a convolutional downsampling block to the input using MaxPooling.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor.\n",
        "        filters (int): Number of filters for the convolution.\n",
        "        kernel_size (int, optional): Size of the convolution kernel. Defaults to 3.\n",
        "        padding (str, optional): Padding strategy, typically 'same'. Defaults to 'same'.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output tensor after applying Conv2D, BatchNormalization, ReLU activation, and MaxPooling.\n",
        "    \"\"\"\n",
        "    x = layers.Conv2D(filters, kernel_size, padding=padding)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    pooled_x = layers.MaxPooling2D(pool_size=(2, 2))(x)  # Apply MaxPooling for downsampling\n",
        "    return pooled_x, x  # Return both the pooled output and the pre-pooled tensor (for skip connection)\n",
        "\n",
        "# A function defining an upsampling block, which increases the spatial dimensions of the input.\n",
        "def upsampling_block(x, skip, filters, kernel_size=3, stride=2, padding='same'):\n",
        "    \"\"\"\n",
        "    Applies a convolutional upsampling block to the input and concatenates with skip connections.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor.\n",
        "        skip (Tensor): Skip connection tensor from the downsampling path.\n",
        "        filters (int): Number of filters for the transposed convolution.\n",
        "        kernel_size (int, optional): Size of the convolution kernel. Defaults to 3.\n",
        "        stride (int, optional): Stride for the transposed convolution. Defaults to 2 for upsampling.\n",
        "        padding (str, optional): Padding strategy, typically 'same'. Defaults to 'same'.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output tensor after applying Conv2DTranspose, concatenating with skip connection,\n",
        "        BatchNormalization, and ReLU activation.\n",
        "    \"\"\"\n",
        "    # Apply a transposed convolution for upsampling\n",
        "    x = layers.Conv2DTranspose(filters, kernel_size, strides=stride, padding=padding)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    # Concatenate the skip connection from the downsampling path\n",
        "    x = layers.Concatenate()([x, skip])\n",
        "    return x\n",
        "\n",
        "# Function to create a U-Net model with skip connections\n",
        "def get_unet_model(img_size, filters, num_classes):\n",
        "    \"\"\"\n",
        "    Builds a U-Net model for image segmentation.\n",
        "\n",
        "    Args:\n",
        "        img_size (tuple): Shape of the input image (height, width, channels).\n",
        "        filters (int): Initial number of filters for the convolutional layers.\n",
        "        num_classes (int): Number of output classes for the segmentation task.\n",
        "\n",
        "    Returns:\n",
        "        Model: A compiled U-Net model for segmentation.\n",
        "    \"\"\"\n",
        "    inputs = tf.keras.Input(shape=img_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    downsampling_layers = []\n",
        "    x = inputs\n",
        "    while x.shape[1] > 8 and x.shape[2] > 8:\n",
        "        x, skip_connection = downsampling_block(x, filters)  # Get both downsampled and skip connection\n",
        "        downsampling_layers.append(skip_connection)  # Save the layer for skip connection\n",
        "        filters *= 2\n",
        "\n",
        "    # Upsampling path with skip connections\n",
        "    for skip in reversed(downsampling_layers):\n",
        "        filters //= 2\n",
        "        x = upsampling_block(x, skip, filters)\n",
        "\n",
        "    # Final output layer\n",
        "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(x)\n",
        "\n",
        "    # Create and return the U-Net model\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "tV2BFtJ12vf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_train, data_val, data_test, loss_function, optimizer, metrics, num_epochs):\n",
        "    # Initialize the variable to track the best loss (used to identify the best model)\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Main training loop for the specified number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Initialize metrics for the current epoch\n",
        "        epoch_metrics = {name: tf.keras.metrics.Mean() for name in metrics.keys()}  # To store per-metric averages\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()  # To compute average loss over the epoch\n",
        "\n",
        "        # Iterate through batches of the training dataset\n",
        "        for batch_index, (x_batch, y_batch) in enumerate(data_train):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Make predictions with the model\n",
        "                y_pred = model(x_batch, training=True)\n",
        "                # Compute the loss for the current batch\n",
        "                loss_value = loss_function(y_batch, y_pred)\n",
        "\n",
        "            # Compute gradients and apply them to update the model weights\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            del tape  # Free up memory by deleting the gradient tape\n",
        "\n",
        "            # Update the average loss and metrics for the current epoch\n",
        "            epoch_loss_avg.update_state(loss_value)\n",
        "            for name, metric in metrics.items():\n",
        "                epoch_metrics[name].update_state(metric(y_batch, y_pred))\n",
        "\n",
        "            # Log progress for each batch in the current epoch\n",
        "            log_message = f\"[Epoch {epoch + 1:03d} - Batch {batch_index + 1:04d}] \"\n",
        "            log_message += f\"Loss: {loss_value.numpy():.4f} | \"\n",
        "            log_message += \" | \".join([f\"{name.capitalize()}: {metric.result().numpy():.4f}\" for name, metric in epoch_metrics.items()])\n",
        "            print(log_message)\n",
        "\n",
        "        # Validation phase: evaluate the model on the validation dataset\n",
        "        val_loss_avg = tf.keras.metrics.Mean()  # To store average validation loss\n",
        "        val_metrics = {name: tf.keras.metrics.Mean() for name in metrics.keys()}  # Store validation metrics\n",
        "\n",
        "        # Disable training-specific behavior for loss function (like dropout)\n",
        "        loss_function.training_phase = False\n",
        "\n",
        "        # Iterate over validation dataset\n",
        "        for x_val, y_val in data_val:\n",
        "            y_val_pred = model(x_val, training=False)  # Make predictions for validation\n",
        "            val_loss_value = loss_function(y_val, y_val_pred)  # Compute validation loss\n",
        "\n",
        "            val_loss_avg.update_state(val_loss_value)\n",
        "            for name, metric in metrics.items():\n",
        "                val_metrics[name].update_state(metric(y_val, y_val_pred))\n",
        "\n",
        "        # Switch loss function back to training mode for the next epoch\n",
        "        loss_function.training_phase = True\n",
        "\n",
        "        # Log validation results for the epoch\n",
        "        log_message = f\"[Epoch {epoch + 1:03d} - Validation] \"\n",
        "        log_message += f\"Loss: {val_loss_avg.result().numpy():.4f} | \"\n",
        "        log_message += \" | \".join([f\"{name.capitalize()}: {metric.result().numpy():.4f}\" for name, metric in val_metrics.items()])\n",
        "        print(log_message)\n",
        "\n",
        "    # Test phase: Evaluate the model on the test dataset after training is completed\n",
        "    loss_function.training_phase = False  # Disable training-specific behaviors for loss calculation\n",
        "\n",
        "    test_loss_avg = tf.keras.metrics.Mean()  # To store average test loss\n",
        "    test_metrics = {name: tf.keras.metrics.Mean() for name in metrics.keys()}  # Store test metrics\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    for x_test, y_test in data_test:\n",
        "        y_test_pred = model.predict(x_test, verbose=0)  # Make predictions for test data\n",
        "        test_loss_value = loss_function(y_test, y_test_pred)  # Compute test loss\n",
        "\n",
        "        test_loss_avg.update_state(test_loss_value)\n",
        "        for name, metric in metrics.items():\n",
        "            test_metrics[name].update_state(metric(y_test, y_test_pred))\n",
        "\n",
        "    # Log the test results\n",
        "    log_message = f\"[Test] \"\n",
        "    log_message += f\"Loss: {test_loss_avg.result().numpy():.4f} | \"\n",
        "    log_message += \" | \".join([f\"{name.capitalize()}: {metric.result().numpy():.4f}\" for name, metric in test_metrics.items()])\n",
        "    print(log_message)\n",
        "\n",
        "    return model  # Return the trained model"
      ],
      "metadata": {
        "id": "u4PnfbNj94ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_breast_dataset():\n",
        "    \"\"\"\n",
        "    Prepares the breast ultrasound dataset by creating necessary directories,\n",
        "    downloading the dataset from Kaggle, extracting images and masks, and moving\n",
        "    them to appropriate folders for further use.\n",
        "\n",
        "    Steps:\n",
        "        1. Create necessary directories for the dataset.\n",
        "        2. Download the dataset from Kaggle using the Kaggle API.\n",
        "        3. Unzip the dataset into a temporary location.\n",
        "        4. Move images and masks from the unzipped folder to dedicated directories.\n",
        "        5. Clean up temporary folders after processing.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create directories for storing the dataset (images and masks)\n",
        "    os.makedirs('Dataset', exist_ok=True)  # Create a 'Dataset' folder if it doesn't already exist\n",
        "    os.makedirs('Breast-Dataset/images', exist_ok=True)  # Create folder for storing images\n",
        "    os.makedirs('Breast-Dataset/masks', exist_ok=True)  # Create folder for storing masks\n",
        "\n",
        "    # Step 2: Download the breast ultrasound dataset from Kaggle (force overwrite if exists)\n",
        "    # The dataset is downloaded as a .zip file.\n",
        "    !kaggle datasets download -d jocelyndumlao/bus-synthetic-dataset --force\n",
        "\n",
        "    # Step 3: Unzip the downloaded dataset into the 'Dataset' folder\n",
        "    # The -o option overwrites existing files, ensuring the latest dataset is used.\n",
        "    !unzip -o \"/content/bus-synthetic-dataset.zip\" -d \"/content/Dataset\"\n",
        "\n",
        "    # Step 4: Move all image files from the extracted dataset folder to the 'Breast-Dataset/images' directory\n",
        "    images_primary_path = '/content/Dataset/BUS Synthetic Dataset/BUS_synthetic_dataset/images'  # Path to extracted images\n",
        "    images_destination_path = '/content/Breast-Dataset/images'  # Destination for images\n",
        "    for item in os.listdir(images_primary_path):\n",
        "        # Skip non-png files (only move .png image files)\n",
        "        if not item.endswith('png'):\n",
        "            continue\n",
        "        # Move the image file from the source folder to the destination folder\n",
        "        source_item = os.path.join(images_primary_path, item)\n",
        "        destination_item = os.path.join(images_destination_path, item)\n",
        "        shutil.move(source_item, destination_item)\n",
        "\n",
        "    # Step 5: Move all mask files from the extracted dataset folder to the 'Breast-Dataset/masks' directory\n",
        "    masks_primary_path = '/content/Dataset/BUS Synthetic Dataset/BUS_synthetic_dataset/masks'  # Path to extracted masks\n",
        "    masks_destination_path = '/content/Breast-Dataset/masks'  # Destination for masks\n",
        "    for item in os.listdir(masks_primary_path):\n",
        "        # Skip non-png files (only move .png mask files)\n",
        "        if not item.endswith('png'):\n",
        "            continue\n",
        "        # Move the mask file from the source folder to the destination folder\n",
        "        source_item = os.path.join(masks_primary_path, item)\n",
        "        destination_item = os.path.join(masks_destination_path, item)\n",
        "        shutil.move(source_item, destination_item)\n",
        "\n",
        "    # Step 6: Clean up by removing the 'Dataset' directory after processing\n",
        "    # Try removing the 'Dataset' folder to save space after moving files\n",
        "    try:\n",
        "        shutil.rmtree('Dataset')  # Remove the temporary 'Dataset' directory\n",
        "    except OSError as e:\n",
        "        # Print error if the directory cannot be removed\n",
        "        print(f\"Error: 'Dataset' could not be removed. {e}\")"
      ],
      "metadata": {
        "id": "bXw_3CsemSP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_breast_dataset()"
      ],
      "metadata": {
        "id": "J-f7-_99vZkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_and_mask(image_path, mask_path):\n",
        "    \"\"\"\n",
        "    Loads an image and its corresponding segmentation mask from file paths.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        mask_path (str): Path to the mask file.\n",
        "\n",
        "    Returns:\n",
        "        image (Tensor): Loaded image tensor with shape [height, width, 1].\n",
        "        mask (Tensor): Loaded mask tensor with shape [height, width, 1].\n",
        "    \"\"\"\n",
        "    # Load the image as a raw byte file and decode it into a grayscale PNG image (1 channel)\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=1)\n",
        "\n",
        "    # Load the mask as a raw byte file and decode it into a grayscale PNG image (1 channel)\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def preprocess_data(image_path, mask_path):\n",
        "    \"\"\"\n",
        "    Preprocesses the image and mask by resizing and normalizing the image,\n",
        "    and converting the mask to one-hot encoded format.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        mask_path (str): Path to the mask file.\n",
        "\n",
        "    Returns:\n",
        "        image (Tensor): Preprocessed image tensor (resized, normalized).\n",
        "        segmentation_mask (Tensor): Preprocessed segmentation mask tensor (resized, one-hot encoded).\n",
        "    \"\"\"\n",
        "    # Load image and mask using helper function\n",
        "    image, segmentation_mask = load_image_and_mask(image_path, mask_path)\n",
        "\n",
        "    # Resize the image and mask to the desired input dimensions (input_height, input_width)\n",
        "    resize_layer_image = keras.layers.Resizing(input_height, input_width, interpolation=\"bilinear\")\n",
        "    resize_layer_mask = keras.layers.Resizing(input_height, input_width, interpolation=\"nearest\")\n",
        "\n",
        "    # Apply the resizing layers to both image and mask\n",
        "    image = resize_layer_image(image)\n",
        "    segmentation_mask = resize_layer_mask(segmentation_mask)\n",
        "\n",
        "    # Normalize the image to a range of [0, 1] by dividing pixel values by 255\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    # Normalize and cast the mask to int32 for one-hot encoding\n",
        "    segmentation_mask = tf.cast(segmentation_mask, tf.float32) / 255.0\n",
        "    segmentation_mask = tf.cast(segmentation_mask, tf.int32)\n",
        "\n",
        "    # One-hot encode the segmentation mask to have shape [height, width, num_classes]\n",
        "    segmentation_mask = tf.one_hot(segmentation_mask[..., 0], num_classes)\n",
        "\n",
        "    return image, segmentation_mask\n",
        "\n",
        "\n",
        "def create_dataset(image_paths, mask_paths, batch_size):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset from a list of image and mask paths,\n",
        "    applies preprocessing, batching, shuffling, and prefetching for efficiency.\n",
        "\n",
        "    Args:\n",
        "        image_paths (list of str): List of image file paths.\n",
        "        mask_paths (list of str): List of mask file paths.\n",
        "        batch_size (int): The size of batches to create.\n",
        "\n",
        "    Returns:\n",
        "        dataset (tf.data.Dataset): A batched and prefetched dataset ready for training.\n",
        "    \"\"\"\n",
        "    # Create a dataset from the image and mask file paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "\n",
        "    # Apply the preprocessing function to the dataset in parallel\n",
        "    dataset = dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Batch the dataset and shuffle it for better generalization\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.shuffle(buffer_size=1024).prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch for faster training\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "VTgNsYSJv2b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory paths where the images and masks are stored\n",
        "image_dir = \"Breast-Dataset/images\"  # Path to the image dataset\n",
        "mask_dir = \"Breast-Dataset/masks\"    # Path to the mask dataset\n",
        "\n",
        "# Input image dimensions and number of segmentation classes\n",
        "input_height = 128  # Height to resize the input images\n",
        "input_width = 128   # Width to resize the input images\n",
        "num_classes = 2     # Number of segmentation classes (e.g., background and tumor)\n",
        "\n",
        "# Hyperparameters for training\n",
        "batch_size = 32        # Number of samples per batch during training\n",
        "num_filters = 32       # Initial number of filters in the convolutional layers\n",
        "learning_rate = 1e-3   # Learning rate for the optimizer\n",
        "num_epochs = 20        # Number of epochs to train the model"
      ],
      "metadata": {
        "id": "iEVpe1AzvnOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the full paths of image and mask files, ensuring only .png files are included\n",
        "image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')])\n",
        "mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir) if fname.endswith('.png')])\n",
        "\n",
        "# Split the dataset into training (70%) and temporary (30%) sets for validation/testing\n",
        "train_images, temp_images, train_masks, temp_masks = train_test_split(\n",
        "    image_paths, mask_paths, test_size=0.3, random_state=42)\n",
        "\n",
        "# Further split the temporary set equally into validation (15%) and test (15%) sets\n",
        "val_images, test_images, val_masks, test_masks = train_test_split(\n",
        "    temp_images, temp_masks, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print the number of images in each subset for confirmation\n",
        "print(f\"Number of training images: {len(train_images)}\")\n",
        "print(f\"Number of validation images: {len(val_images)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")"
      ],
      "metadata": {
        "id": "vjfDwy9Bv8-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets for training, validation, and testing by processing the respective image and mask paths\n",
        "data_train = create_dataset(train_images, train_masks, batch_size)  # Create training dataset\n",
        "data_val = create_dataset(val_images, val_masks, batch_size)        # Create validation dataset\n",
        "data_test = create_dataset(test_images, test_masks, batch_size)     # Create testing dataset"
      ],
      "metadata": {
        "id": "alNHFtiHvcNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the next batch of images and masks from the training dataset\n",
        "images, masks = next(iter(data_train))\n",
        "\n",
        "# Convert the first 16 images and masks to NumPy arrays and ensure the data type is float32\n",
        "images_np = images[:16].numpy().astype(\"float32\")\n",
        "masks_np = masks[:16].numpy().astype(\"float32\")\n",
        "\n",
        "# Create a 4x4 grid of subplots for visualization\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "\n",
        "# Loop through the first 16 images to display them along with their masks\n",
        "for i in range(16):\n",
        "    ax = axs[i // 4, i % 4]  # Get the appropriate subplot axis\n",
        "    ax.imshow(images_np[i], cmap=\"magma\")  # Display the image using the 'magma' colormap\n",
        "    ax.imshow(np.argmax(masks_np[i].squeeze(), axis=-1), cmap=\"gray\", alpha=0.5)  # Overlay the mask with transparency\n",
        "    ax.axis(\"off\")  # Hide the axis ticks and labels\n",
        "\n",
        "# Adjust layout to prevent overlap and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uYe9y-YTygLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a U-Net model with the specified input dimensions, number of filters, and number of output classes\n",
        "model = get_unet_model((input_height, input_width, 1), filters=num_filters, num_classes=num_classes)\n",
        "\n",
        "# Display the model architecture summary, including layer details and the total number of parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "G_ySBvjlxdep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the custom loss function for training\n",
        "loss_function = DynamicMemoryFusionLoss(\n",
        "    # List of loss functions to be combined in the final loss computation\n",
        "    loss_functions=[\n",
        "        tf.keras.losses.CategoricalCrossentropy(),     # Standard categorical cross-entropy loss\n",
        "        iou_loss,                                      # Intersection over Union (IoU) loss\n",
        "        dice_loss                                      # Dice coefficient loss\n",
        "    ],\n",
        "    history_size=50,                                   # Number of past loss values to consider for dynamic weighting\n",
        "    history_size_max=150,                              # Maximum size of the loss history for tracking purposes\n",
        "    batch_track_steps=100,                             # Number of batches to track for auxiliary loss function decay rate updates\n",
        "    auxiliary_loss_function=class_balanced_dice_loss,  # Auxiliary loss function to address class imbalance\n",
        "    gama=5,                                            # Hyperparameter for controlling the contribution of the auxiliary loss\n",
        "    decay_rate=0.05,                                   # Decay rate for the auxiliary loss over time\n",
        "    weighting_method='var',                            # Method used for weighting losses; in this case, based on variance\n",
        "    training_phase=True                                # Flag indicating whether the model is in training mode\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the Adam optimizer with a specified learning rate for model training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "ysDAMwst_T4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to store various evaluation metrics for model performance\n",
        "metrics = {\n",
        "    \"dice\": dice,                               # Dice coefficient metric for measuring overlap between predicted and true masks\n",
        "    \"iou\": iou,                                 # Intersection over Union metric for evaluating segmentation accuracy\n",
        "    \"f1_score\": f1_score,                       # F1 Score to balance precision and recall\n",
        "    \"precision\": precision,                     # Precision metric to evaluate the correctness of positive predictions\n",
        "    \"recall\": recall,                           # Recall metric to assess the model's ability to identify true positives\n",
        "    \"cb-dice\": class_balanced_dice_score        # Class Balanced Dice Score for handling class imbalances in segmentation tasks\n",
        "}"
      ],
      "metadata": {
        "id": "yY82y3lS7WCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " Train the model using the specified training, validation, and test datasets,\n",
        " along with the defined loss function, optimizer, metrics, and number of epochs.\n",
        "\"\"\"\n",
        "model = train_model(model=model,                   # The model to be trained\n",
        "                    data_train=data_train,         # Training dataset containing image-mask pairs\n",
        "                    data_val=data_val,             # Validation dataset for evaluating model performance during training\n",
        "                    data_test=data_test,           # Test dataset for final evaluation after training\n",
        "                    loss_function=loss_function,   # Custom loss function to guide the training process\n",
        "                    optimizer=optimizer,           # Optimizer to update model weights during training\n",
        "                    metrics=metrics,               # Dictionary of metrics for tracking model performance\n",
        "                    num_epochs=num_epochs)         # Number of epochs for which to train the model"
      ],
      "metadata": {
        "id": "nFxpqpWzAI3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a batch of images and corresponding masks from the test dataset\n",
        "images, masks = next(iter(data_test))\n",
        "\n",
        "# Convert the first 4 images to NumPy arrays and ensure they are of type float32\n",
        "images_np = images[:4].numpy().astype(\"float32\")\n",
        "\n",
        "# Generate predictions for the input images using the trained model\n",
        "pred_masks = model.predict(images, verbose=0)\n",
        "\n",
        "# Create a 2x2 grid of subplots for visualization\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "# Loop through the first 4 images and display them along with their predicted masks\n",
        "for i in range(4):\n",
        "    ax = axs[i // 2, i % 2]  # Determine subplot position\n",
        "    ax.imshow(images_np[i], cmap=\"magma\")  # Display the original image\n",
        "    ax.imshow(np.argmax(pred_masks[i].squeeze(), axis=-1), cmap=\"gray\", alpha=0.5)  # Overlay the predicted mask\n",
        "    ax.axis(\"off\")  # Hide the axis ticks and labels\n",
        "\n",
        "# Adjust the layout to prevent overlap and display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-3lZdID00Zde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters and settings for the model training\n",
        "num_classes = 4                # Number of classes for segmentation (including background)\n",
        "input_height = 128             # Height of the input images\n",
        "input_width = 128              # Width of the input images\n",
        "learning_rate = 1e-3           # Learning rate for the optimizer\n",
        "num_epochs = 20                # Number of epochs for training\n",
        "batch_size = 32                # Number of samples per batch\n",
        "num_filters = 32               # Initial number of filters in the convolutional layers\n",
        "shuffle = True                 # Flag to indicate whether to shuffle the dataset during training"
      ],
      "metadata": {
        "id": "6-ko_f35venm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " Load the Oxford Pets dataset from TensorFlow Datasets (TFDS)\n",
        " The dataset is split into training, validation, and testing sets\n",
        " The images are loaded in batches of the specified size, and the order of the files can be shuffled\n",
        "\"\"\"\n",
        "(data_train, data_val, data_test) = tfds.load(\n",
        "    name = \"oxford_iiit_pet\",                        # Name of the dataset to load\n",
        "    split = [\"train[:80%]\", \"train[80%:]\", \"test\"],  # Splits the training set into 80% for training and 20% for validation\n",
        "    batch_size = batch_size,                         # Number of samples per batch\n",
        "    shuffle_files = shuffle,                         # Flag to indicate whether to shuffle the dataset files\n",
        ")\n"
      ],
      "metadata": {
        "id": "FbEcAfBVvuny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(section):\n",
        "    # Extract the image and segmentation mask from the input section\n",
        "    image = section[\"image\"]\n",
        "    segmentation_mask = section[\"segmentation_mask\"]\n",
        "\n",
        "    # Create resizing layers for both image and segmentation mask\n",
        "    resize_layer_image = keras.layers.Resizing(input_height, input_width, interpolation=\"bilinear\")\n",
        "    resize_layer_mask = keras.layers.Resizing(input_height, input_width, interpolation=\"nearest\")\n",
        "\n",
        "    # Resize the image and segmentation mask\n",
        "    image = resize_layer_image(image)\n",
        "    segmentation_mask = resize_layer_mask(segmentation_mask)\n",
        "\n",
        "    # Normalize the image to the range [0, 1]\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    # Convert the segmentation mask to one-hot encoded format\n",
        "    # Use the last channel of the mask and cast to int32 for one-hot encoding\n",
        "    segmentation_mask = tf.one_hot(tf.cast(segmentation_mask[..., -1], tf.int32), num_classes)\n",
        "\n",
        "    return image, segmentation_mask\n",
        "\n",
        "# Preprocess the training dataset with shuffling and prefetching for better performance\n",
        "data_train = (\n",
        "    data_train.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing function\n",
        "    .shuffle(buffer_size=1024)  # Shuffle the dataset to ensure randomness\n",
        "    .prefetch(buffer_size=1024)  # Prefetch batches to improve performance\n",
        ")\n",
        "\n",
        "# Preprocess the validation dataset similarly to the training dataset\n",
        "data_val = (\n",
        "    data_val.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing function\n",
        "    .shuffle(buffer_size=1024)  # Shuffle the dataset to ensure randomness\n",
        "    .prefetch(buffer_size=1024)  # Prefetch batches to improve performance\n",
        ")\n",
        "\n",
        "# Preprocess the test dataset similarly to the training dataset\n",
        "data_test = (\n",
        "    data_test.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)  # Apply preprocessing function\n",
        "    .shuffle(buffer_size=1024)  # Shuffle the dataset to ensure randomness\n",
        "    .prefetch(buffer_size=1024)  # Prefetch batches to improve performance\n",
        ")"
      ],
      "metadata": {
        "id": "Fql9knO-vyOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images and masks from the training dataset\n",
        "images, masks = next(iter(data_train))\n",
        "\n",
        "# Convert the first 16 images and masks to NumPy arrays with float32 type\n",
        "images_np = images[:16].numpy().astype(\"float32\")\n",
        "masks_np = masks[:16].numpy().astype(\"float32\")\n",
        "\n",
        "# Create a 4x4 grid for displaying images and their corresponding masks\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "\n",
        "# Iterate over the first 16 images and masks to display them\n",
        "for i in range(16):\n",
        "    ax = axs[i // 4, i % 4]  # Get the appropriate subplot axis\n",
        "    ax.imshow(images_np[i])  # Display the image\n",
        "    # Overlay the predicted mask on top of the image using a colormap\n",
        "    ax.imshow(np.argmax(masks_np[i].squeeze(), axis=-1), cmap=\"turbo\", alpha=0.5)\n",
        "    ax.axis(\"off\")  # Hide the axis for better visualization\n",
        "\n",
        "# Adjust layout to avoid overlap and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "93db48OP4u5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input image shape: {np.shape(images)[1:]}\")\n",
        "print(f\"Mask shape: {np.shape(masks)[1:]}\")\n",
        "print(f\"Number of classes: {np.shape(masks)[-1]}\")"
      ],
      "metadata": {
        "id": "3IoOKkIjwfxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a U-Net model with the specified input dimensions, number of filters, and number of output classes\n",
        "model = get_unet_model((input_height, input_width, 3), filters=num_filters, num_classes=num_classes)\n",
        "\n",
        "# Display the model architecture summary, including layer details and the total number of parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jDeIPKV_1YTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the custom loss function for training\n",
        "loss_function = DynamicMemoryFusionLoss(\n",
        "    # List of loss functions to be combined in the final loss computation\n",
        "    loss_functions=[\n",
        "        tf.keras.losses.CategoricalCrossentropy(),     # Standard categorical cross-entropy loss\n",
        "        iou_loss,                                      # Intersection over Union (IoU) loss\n",
        "        dice_loss                                      # Dice coefficient loss\n",
        "    ],\n",
        "    history_size=100,                                  # Number of past loss values to consider for dynamic weighting\n",
        "    history_size_max=250,                              # Maximum size of the loss history for tracking purposes\n",
        "    batch_track_steps=100,                             # Number of batches to track for auxiliary loss function decay rate updates\n",
        "    auxiliary_loss_function=class_balanced_dice_loss,  # Auxiliary loss function to address class imbalance\n",
        "    gama=5,                                            # Hyperparameter for controlling the contribution of the auxiliary loss\n",
        "    decay_rate=0.05,                                   # Decay rate for the auxiliary loss over time\n",
        "    weighting_method='var',                            # Method used for weighting losses; in this case, based on variance\n",
        "    training_phase=True                                # Flag indicating whether the model is in training mode\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the Adam optimizer with a specified learning rate for model training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "fjcG_xw_BkMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to store various evaluation metrics for model performance\n",
        "metrics = {\n",
        "    \"dice\": dice,                               # Dice coefficient metric for measuring overlap between predicted and true masks\n",
        "    \"iou\": iou,                                 # Intersection over Union metric for evaluating segmentation accuracy\n",
        "    \"f1_score\": f1_score,                       # F1 Score to balance precision and recall\n",
        "    \"precision\": precision,                     # Precision metric to evaluate the correctness of positive predictions\n",
        "    \"recall\": recall,                           # Recall metric to assess the model's ability to identify true positives\n",
        "    \"cb-dice\": class_balanced_dice_score        # Class Balanced Dice Score for handling class imbalances in segmentation tasks\n",
        "}"
      ],
      "metadata": {
        "id": "Zvw4wQhB1YUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " Train the model using the specified training, validation, and test datasets,\n",
        " along with the defined loss function, optimizer, metrics, and number of epochs.\n",
        "\"\"\"\n",
        "model = train_model(model=model,                   # The model to be trained\n",
        "                    data_train=data_train,         # Training dataset containing image-mask pairs\n",
        "                    data_val=data_val,             # Validation dataset for evaluating model performance during training\n",
        "                    data_test=data_test,           # Test dataset for final evaluation after training\n",
        "                    loss_function=loss_function,   # Custom loss function to guide the training process\n",
        "                    optimizer=optimizer,           # Optimizer to update model weights during training\n",
        "                    metrics=metrics,               # Dictionary of metrics for tracking model performance\n",
        "                    num_epochs=num_epochs)         # Number of epochs for which to train the model"
      ],
      "metadata": {
        "id": "uIzNtiXO1YUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images and masks from the test dataset\n",
        "images, masks = next(iter(data_test))\n",
        "\n",
        "# Convert the first 16 images to NumPy arrays with float32 type\n",
        "images_np = images[:16].numpy().astype(\"float32\")\n",
        "# Generate predicted masks using the trained model on the test images\n",
        "pred_masks = model.predict(images, verbose=0)\n",
        "\n",
        "# Create a 4x4 grid for displaying images and their corresponding predicted masks\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "\n",
        "# Iterate over the first 16 images and their predicted masks for display\n",
        "for i in range(16):\n",
        "    ax = axs[i // 4, i % 4]  # Get the appropriate subplot axis\n",
        "    ax.imshow(images_np[i])  # Display the original image\n",
        "    # Overlay the predicted mask on top of the image using a colormap\n",
        "    ax.imshow(np.argmax(pred_masks[i].squeeze(), axis=-1), cmap=\"turbo\", alpha=0.5)\n",
        "    ax.axis(\"off\")  # Hide the axis for better visualization\n",
        "\n",
        "# Adjust layout to avoid overlap and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h3-r68FCBzC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YymdoSoPyD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
